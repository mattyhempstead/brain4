{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiMZMZYYpFA9"
   },
   "source": [
    "# Final Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZZESMLfqCMX"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zd11uFJxp8Dd"
   },
   "source": [
    "## 1. Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ri2fXIvFp8GH"
   },
   "source": [
    "## 2. Aim and Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYqcpOKnp8Ig"
   },
   "source": [
    "### 2.1. Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZzWojd-rBmf"
   },
   "source": [
    "### 2.2. Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGFpT0m-rBxy"
   },
   "source": [
    "## 3. Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwcexQTIrB52"
   },
   "source": [
    "### 3.1. Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "l7Bv-0EM83z5"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aCgoTZLc59ic",
    "outputId": "94fa0c97-a3fd-4d31-f09f-3ffafa87af2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from data_plot import plot_samples_events_individual\n",
    "\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENT_ID_MAP = {\n",
    "    None: 0,\n",
    "    \"L\": 1,\n",
    "    \"R\": 2,\n",
    "    \"S\": 3,\n",
    "}\n",
    "EVENT_ID_LETTER_MAP = {EVENT_ID_MAP[i]:i for i in EVENT_ID_MAP}\n",
    "\n",
    "EVENT_COLOR_MAP = {\n",
    "    None: \"black\",\n",
    "    \"L\": \"red\",\n",
    "    \"R\": \"blue\",\n",
    "    \"S\": \"green\",\n",
    "}\n",
    "\n",
    "EVENT_ID_NAME_MAP = {\n",
    "    0: \"Nothing\",\n",
    "    1: \"Left Wink\",\n",
    "    2: \"Right Wink\",\n",
    "    3: \"Dbl Blink\",\n",
    "}\n",
    "\n",
    "BRAINBOX_SAMPLE_RATE = 10000\n",
    "DOWNSAMPLE_RATE = 100\n",
    "\n",
    "EVENT_LENGTH = 2 # length of a given event sequence in seconds\n",
    "EVENT_SAMPLE_COUNT = int(EVENT_LENGTH * BRAINBOX_SAMPLE_RATE / DOWNSAMPLE_RATE) # size of event in samples\n",
    "\n",
    "EVENT_START = -0.75\n",
    "EVENT_START_OFFSET = int(EVENT_START * BRAINBOX_SAMPLE_RATE / DOWNSAMPLE_RATE)\n",
    "\n",
    "EVENT_END = -0.25\n",
    "EVENT_END_OFFSET = int(EVENT_END * BRAINBOX_SAMPLE_RATE / DOWNSAMPLE_RATE)\n",
    "\n",
    "INPUT_SHAPE = (EVENT_SAMPLE_COUNT,)\n",
    "OUTPUT_SHAPE = len(EVENT_ID_MAP)  # number of categories (including None)\n",
    "\n",
    "\n",
    "EVENTS_PATH = \"../src/data_collection/data/events/\"\n",
    "SAMPLES_PATH = \"../src/data_collection/data/waves/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAMES_ALL = [\n",
    "    \"DATA_2022-05-13_Josh_0001_3_1652400625\",\n",
    "#     \"DATA_2022-05-13_Josh_0001_3_1652400939\",\n",
    "    \"DATA_2022-05-13_Josh_0001_4_1652401267\",\n",
    "#     \"DATA_2022-05-13_Josh_0001_4_1652401740\",\n",
    "    \"DATA_2022-05-13_Josh_0001_5_1652405337\",\n",
    "#     \"DATA_2022-05-13_Josh_0001_5_1652405637\",\n",
    "#     \"DATA_2022-05-13_Josh_0001_6_1652406023\",\n",
    "#     \"DATA_2022-05-13_Josh_0001_6_1652406202\",\n",
    "#     \"DATA_2022-05-13_Josh_0001_7_1652406589\",\n",
    "#     \"DATA_2022-05-13_Josh_0001_7_1652406788\",\n",
    "#     \"DATA_2022-05-13_Josh_0001_8_1652407331\",\n",
    "#     \"DATA_2022-05-13_Josh_0001_8_1652407508\",\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvrrURLQpFDk"
   },
   "source": [
    "### 3.2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessing\n",
      "Loading ML data from 1 files\n",
      "\n",
      "Transforming data into individual sequences...\n",
      "Transformed into 19047 sequences of size 200\n",
      "\n",
      "Combined 1 files into 19047 sequences of size 200\n",
      "Loading ML data from 1 files\n",
      "\n",
      "Transforming data into individual sequences...\n",
      "Transformed into 21609 sequences of size 200\n",
      "\n",
      "Combined 1 files into 21609 sequences of size 200\n",
      "Loading ML data from 1 files\n",
      "\n",
      "Transforming data into individual sequences...\n",
      "Transformed into 24467 sequences of size 200\n",
      "\n",
      "Combined 1 files into 24467 sequences of size 200\n"
     ]
    }
   ],
   "source": [
    "from data_ml import get_ml_data\n",
    "\n",
    "print(\"Data Preprocessing\")\n",
    "files_data_all = []\n",
    "files_labels_all = []\n",
    "\n",
    "for file_name in FILE_NAMES_ALL:\n",
    "    f_data_all, f_labels_all = get_ml_data(\n",
    "        events_path = EVENTS_PATH,\n",
    "        samples_path = SAMPLES_PATH,\n",
    "        file_names = [file_name],\n",
    "\n",
    "        event_id_map = EVENT_ID_MAP,\n",
    "        event_color_map = EVENT_COLOR_MAP,\n",
    "\n",
    "        event_sample_count = EVENT_SAMPLE_COUNT,\n",
    "        event_start = EVENT_START,\n",
    "        event_end = EVENT_END,\n",
    "\n",
    "        downsample_rate = DOWNSAMPLE_RATE,\n",
    "        shuffle_data = False,\n",
    "        filter_data = True,\n",
    "    )\n",
    "\n",
    "    files_data_all.append(f_data_all)\n",
    "    files_labels_all.append(f_labels_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RNgpn2dpFGW"
   },
   "source": [
    "### 3.3. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-01 17:52:10.682212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-01 17:52:10.703942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-01 17:52:10.704101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 200)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 200)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                12864     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                1040      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,972\n",
      "Trainable params: 13,972\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-01 17:52:10.704753: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-01 17:52:10.705488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-01 17:52:10.705611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-01 17:52:10.705709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-01 17:52:10.947247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-01 17:52:10.947396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-01 17:52:10.947515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-01 17:52:10.947601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1831 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 970, pci bus id: 0000:01:00.0, compute capability: 5.2\n",
      "2022-06-01 17:52:10.947807: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def get_model_ann():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=INPUT_SHAPE),\n",
    "        tf.keras.layers.Dropout(.50, input_shape=INPUT_SHAPE),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(16, activation='relu'),\n",
    "        tf.keras.layers.Dense(OUTPUT_SHAPE)\n",
    "    ])\n",
    "    return model\n",
    "get_model_ann().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "def get_model_svm():\n",
    "    clf = svm.SVC()\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRFClassifier\n",
    "\n",
    "# Hyper-Parameters\n",
    "RF_N_TREES = [10,50,100,500,1000,5000]\n",
    "RF_N_FEATURES = [x for x in np.arange(0.1, 1.1, 0.1)]\n",
    "\n",
    "def get_model_rf(num_trees=50, num_features=0.1):\n",
    "    return XGBRFClassifier(\n",
    "        n_estimators=num_trees,\n",
    "        subsample=0.9,\n",
    "        colsample_bynode=num_features\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGYMbnF6pFJQ"
   },
   "source": [
    "### 3.4. Evaluation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN_EPOCHS = 3\n",
    "ANN_OPTIMIZER = 'adam'\n",
    "\n",
    "def train_model_ann(model, train_data, train_labels, test_data, test_labels):\n",
    "    print(\"Training ANN with\", len(train_labels), \"samples\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Trains model and returns history dict\n",
    "    model.compile(\n",
    "        optimizer=ANN_OPTIMIZER,\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        train_labels,\n",
    "        epochs=ANN_EPOCHS,\n",
    "        validation_data=(test_data, test_labels)\n",
    "    #     batch_size=16\n",
    "    )\n",
    "\n",
    "    print(f\"Completed training ANN in {time.time()-start_time:.2f}s\")\n",
    "\n",
    "    print(f\"Generating predictions for test set\", len(test_labels))\n",
    "    start_time = time.time()\n",
    "    \n",
    "    test_out = model.predict(test_data)\n",
    "    test_probs = tf.nn.softmax(test_out, axis=1)\n",
    "    test_pred = np.argmax(test_probs, axis=1)\n",
    "    \n",
    "    print(f\"Generated predictions in {time.time()-start_time:.2f}s\")\n",
    "\n",
    "    # Return predictions for test set\n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_svm(model, train_data, train_labels, test_data, test_labels):\n",
    "    subset = np.random.random(len(train_data)) < 0.1\n",
    "    print(\"Training SVM with\", sum(subset), \"samples\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.fit(train_data[subset], train_labels[subset])\n",
    "    \n",
    "    print(f\"Completed training SVM in {time.time()-start_time:.2f}s\")\n",
    "\n",
    "    print(f\"Generating predictions for test set\", len(test_labels))\n",
    "    start_time = time.time()\n",
    "    \n",
    "    test_pred = model.predict(test_data)\n",
    "\n",
    "    print(f\"Generated predictions in {time.time()-start_time:.2f}s\")\n",
    "    \n",
    "    # Return predictions for test set\n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Training and Testing\n",
    "def train_model_xgbrf(model, train_data, train_labels, test_data, test_labels):\n",
    "    print(\"Training XGBRandomForest with\", len(train_data), \"samples\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.fit(train_data, train_labels)\n",
    "    \n",
    "    print(f\"Completed training XGBRandomForest in {time.time()-start_time:.2f}s\")\n",
    "    \n",
    "    print(f\"Generating predictions for test set\", len(test_labels))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    test_pred = model.predict(test_data)\n",
    "    \n",
    "    print(f\"Generated predictions in {time.time()-start_time:.2f}s\")\n",
    "\n",
    "    # Return predictions for test set\n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(label_pred, label_true):\n",
    "    # Rows are \"real\" labels\n",
    "    # Columns are \"predicted\" labels\n",
    "    conf = tf.math.confusion_matrix(\n",
    "        label_pred,\n",
    "        label_true\n",
    "    )\n",
    "\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1/3\n",
      "(46076, 200) (46076,)\n",
      "(19047, 200) (19047,)\n",
      "Training ANN with 46076 samples\n",
      "Epoch 1/3\n",
      "1440/1440 [==============================] - 2s 1ms/step - loss: 0.2374 - accuracy: 0.9127 - val_loss: 0.2338 - val_accuracy: 0.9211\n",
      "Epoch 2/3\n",
      "1440/1440 [==============================] - 2s 1ms/step - loss: 0.1497 - accuracy: 0.9441 - val_loss: 0.1825 - val_accuracy: 0.9391\n",
      "Epoch 3/3\n",
      "1440/1440 [==============================] - 2s 1ms/step - loss: 0.1233 - accuracy: 0.9532 - val_loss: 0.1624 - val_accuracy: 0.9466\n",
      "Completed training ANN in 5.61s\n",
      "Generating predictions for test set 19047\n",
      "Generated predictions in 0.35s\n",
      "Training SVM with 4627 samples\n",
      "Completed training SVM in 0.82s\n",
      "Generating predictions for test set 10000\n",
      "Generated predictions in 2.47s\n",
      "Training XGBRandomForest with 46076 samples\n",
      "Completed training XGBRandomForest in 2.74s\n",
      "Generating predictions for test set 19047\n",
      "Generated predictions in 0.01s\n",
      "Done fold in 12.10s\n",
      "\n",
      "Fold #2/3\n",
      "(43514, 200) (43514,)\n",
      "(21609, 200) (21609,)\n",
      "Training ANN with 43514 samples\n",
      "Epoch 1/3\n",
      "1360/1360 [==============================] - 2s 1ms/step - loss: 0.2324 - accuracy: 0.9168 - val_loss: 0.1881 - val_accuracy: 0.9363\n",
      "Epoch 2/3\n",
      "1360/1360 [==============================] - 2s 1ms/step - loss: 0.1372 - accuracy: 0.9491 - val_loss: 0.1729 - val_accuracy: 0.9420\n",
      "Epoch 3/3\n",
      "1360/1360 [==============================] - 2s 1ms/step - loss: 0.1195 - accuracy: 0.9540 - val_loss: 0.1658 - val_accuracy: 0.9476\n",
      "Completed training ANN in 5.05s\n",
      "Generating predictions for test set 21609\n",
      "Generated predictions in 0.34s\n",
      "Training SVM with 4378 samples\n",
      "Completed training SVM in 0.77s\n",
      "Generating predictions for test set 10000\n"
     ]
    }
   ],
   "source": [
    "CV_K = len(FILE_NAMES_ALL)\n",
    "\n",
    "cv_conf_ann = []\n",
    "cv_conf_svm = []\n",
    "cv_conf_rf = []\n",
    "\n",
    "for k in range(CV_K):\n",
    "    print(f\"Fold #{k+1}/{CV_K}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get training data/labels\n",
    "    train_data = np.concatenate(files_data_all[0:k] + files_data_all[k+1:])\n",
    "    train_labels = np.concatenate(files_labels_all[0:k] + files_labels_all[k+1:])\n",
    "    print(train_data.shape, train_labels.shape)\n",
    "    \n",
    "    test_data = files_data_all[k]\n",
    "    test_labels = files_labels_all[k]\n",
    "    print(test_data.shape, test_labels.shape)\n",
    "\n",
    "    # Train and test ANN\n",
    "    model_ann = get_model_ann()\n",
    "    test_pred_ann = train_model_ann(model_ann, train_data, train_labels, test_data, test_labels)\n",
    "    conf_ann = confusion_matrix(test_pred_ann, test_labels)\n",
    "    cv_conf_ann.append(conf_ann)\n",
    "    \n",
    "    # Train and test SVM\n",
    "    model_svm = get_model_svm()\n",
    "    test_pred_svm = train_model_svm(model_svm, train_data, train_labels, test_data[:10000], test_labels[:10000])\n",
    "    conf_svm = confusion_matrix(test_pred_svm, test_labels[:10000])\n",
    "    cv_conf_svm.append(conf_svm)\n",
    "    \n",
    "    # Train and Test XGBRandomForest\n",
    "    model_xgbrf = get_model_rf()\n",
    "    test_pred_xgbrf = train_model_xgbrf(model_xgbrf, train_data, train_labels, test_data, test_labels)\n",
    "    conf_xgbrf = confusion_matrix(test_pred_xgbrf, test_labels)\n",
    "    cv_conf_rf.append(conf_xgbrf)\n",
    "\n",
    "    print(f\"Done fold in {time.time() - start_time:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_conf_ann)\n",
    "print(cv_conf_svm)\n",
    "print(cv_conf_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2YPuEpSrTrv"
   },
   "source": [
    "## 4. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxCTdo0_rTuH"
   },
   "source": [
    "### 4.1. Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(conf):\n",
    "    print(conf)\n",
    "    \n",
    "    overall_total = sum(sum(conf))\n",
    "    overall_correct = sum([conf[i][i] for i in range(len(conf))])\n",
    "    print(f\"Overall accuracy: {100*overall_correct/overall_total:.2f}% ({overall_correct}/{overall_total})\")\n",
    "    \n",
    "    fp = sum(conf[0][1:])\n",
    "    tp = sum(sum(conf[1:]))\n",
    "    print(f\"False positives {fp}\")\n",
    "    print(f\"True positives {tp}\")\n",
    "    print(f\"False discovery (fp/(tp+fp)): {fp/(fp+tp):.4f} ({fp}/{fp+tp})\")\n",
    "    \n",
    "    for i in range(len(conf)):\n",
    "        letter = str(EVENT_ID_LETTER_MAP[i])[0]\n",
    "        \n",
    "        total = sum(conf[i])\n",
    "        correct = conf[i][i]\n",
    "        acc_total = 100*correct/total\n",
    "        \n",
    "        s = f\"Event {letter} ({i}) accuracy: {correct:6}/{total: <6} (t_acc {acc_total:5.2f}%)\"\n",
    "        if i > 0:\n",
    "            acc_event = 100*correct/(total - conf[i][0])\n",
    "            s += f\" (e_acc {acc_event:5.2f}%)\"\n",
    "        print(s)\n",
    "        \n",
    "    print(\"\")\n",
    "        \n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_conf_ann = np.sum(cv_conf_ann, axis=0)\n",
    "print_confusion_matrix(total_conf_ann)\n",
    "\n",
    "total_conf_svm = np.sum(cv_conf_svm, axis=0)\n",
    "print_confusion_matrix(total_conf_svm)\n",
    "\n",
    "total_conf_rf = np.sum(cv_conf_rf, axis=0)\n",
    "print_confusion_matrix(total_conf_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9z3anWmrTwe"
   },
   "source": [
    "### 4.2. Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKFJra9urTyp"
   },
   "source": [
    "## 5. Discussion and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sb2q5DyrnY_"
   },
   "source": [
    "### 5.1. Issues Addressed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5Abg6FlrnbR"
   },
   "source": [
    "### 5.2. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGVcADPHrnd5"
   },
   "source": [
    "## 6. Student Contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0X4cmXHrngc"
   },
   "source": [
    "### 6.1. Matty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bafpxraprni8"
   },
   "source": [
    "### 6.2. Ashwin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zjRi9i3r9nn"
   },
   "source": [
    "### 6.3. Marcus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaTrmwj7r9qK"
   },
   "source": [
    "### 6.4. Alex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HS1Q0Pxr9sZ"
   },
   "source": [
    "### 6.5. Jingyu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ukHvavmr9ui"
   },
   "source": [
    "### 6.9. Josh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bZM8plir9xL"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8q57R1Hkr9zj"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjMRmECbr92D"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BoyYYZyr94d"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Paog5ZoCr97E"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pd0gRmUr99z"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkXy1VyMr-BO"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A02CJpxarnld"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OjUMV1vkrnoP"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Final Report.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
