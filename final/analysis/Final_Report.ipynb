{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiMZMZYYpFA9"
   },
   "source": [
    "# Final Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZZESMLfqCMX"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zd11uFJxp8Dd"
   },
   "source": [
    "## 1. Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ri2fXIvFp8GH"
   },
   "source": [
    "## 2. Aim and Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYqcpOKnp8Ig"
   },
   "source": [
    "### 2.1. Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZzWojd-rBmf"
   },
   "source": [
    "### 2.2. Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGFpT0m-rBxy"
   },
   "source": [
    "## 3. Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwcexQTIrB52"
   },
   "source": [
    "### 3.1. Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "l7Bv-0EM83z5"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aCgoTZLc59ic",
    "outputId": "94fa0c97-a3fd-4d31-f09f-3ffafa87af2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from data_plot import plot_samples_events_individual\n",
    "\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENT_ID_MAP = {\n",
    "    None: 0,\n",
    "    \"L\": 1,\n",
    "    \"R\": 2,\n",
    "    \"S\": 3,\n",
    "}\n",
    "EVENT_ID_LETTER_MAP = {EVENT_ID_MAP[i]:i for i in EVENT_ID_MAP}\n",
    "\n",
    "EVENT_COLOR_MAP = {\n",
    "    None: \"black\",\n",
    "    \"L\": \"red\",\n",
    "    \"R\": \"blue\",\n",
    "    \"S\": \"green\",\n",
    "}\n",
    "\n",
    "EVENT_ID_NAME_MAP = {\n",
    "    0: \"Nothing\",\n",
    "    1: \"Left Wink\",\n",
    "    2: \"Right Wink\",\n",
    "    3: \"Dbl Blink\",\n",
    "}\n",
    "\n",
    "BRAINBOX_SAMPLE_RATE = 10000\n",
    "DOWNSAMPLE_RATE = 100\n",
    "\n",
    "EVENT_LENGTH = 2 # length of a given event sequence in seconds\n",
    "EVENT_SAMPLE_COUNT = int(EVENT_LENGTH * BRAINBOX_SAMPLE_RATE / DOWNSAMPLE_RATE) # size of event in samples\n",
    "\n",
    "EVENT_START = -0.75\n",
    "EVENT_START_OFFSET = int(EVENT_START * BRAINBOX_SAMPLE_RATE / DOWNSAMPLE_RATE)\n",
    "\n",
    "EVENT_END = -0.25\n",
    "EVENT_END_OFFSET = int(EVENT_END * BRAINBOX_SAMPLE_RATE / DOWNSAMPLE_RATE)\n",
    "\n",
    "INPUT_SHAPE = (EVENT_SAMPLE_COUNT,)\n",
    "OUTPUT_SHAPE = len(EVENT_ID_MAP)  # number of categories (including None)\n",
    "\n",
    "\n",
    "EVENTS_PATH = \"../src/data_collection/data/events/\"\n",
    "SAMPLES_PATH = \"../src/data_collection/data/waves/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAMES_ALL = [\n",
    "    \"DATA_2022-05-13_Josh_0001_3_1652400625\",\n",
    "#     \"DATA_2022-05-13_Josh_0001_3_1652400939\",\n",
    "    \"DATA_2022-05-13_Josh_0001_4_1652401267\",\n",
    "#     \"DATA_2022-05-13_Josh_0001_4_1652401740\",\n",
    "    \"DATA_2022-05-13_Josh_0001_5_1652405337\",\n",
    "#     \"DATA_2022-05-13_Josh_0001_5_1652405637\",\n",
    "#     \"DATA_2022-05-13_Josh_0001_6_1652406023\",\n",
    "#     \"DATA_2022-05-13_Josh_0001_6_1652406202\",\n",
    "#     \"DATA_2022-05-13_Josh_0001_7_1652406589\",\n",
    "#     \"DATA_2022-05-13_Josh_0001_7_1652406788\",\n",
    "#     \"DATA_2022-05-13_Josh_0001_8_1652407331\",\n",
    "#     \"DATA_2022-05-13_Josh_0001_8_1652407508\",\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvrrURLQpFDk"
   },
   "source": [
    "### 3.2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessing\n",
      "Loading ML data from 1 files\n",
      "\n",
      "Transforming data into individual sequences...\n",
      "Transformed into 19047 sequences of size 200\n",
      "\n",
      "Combined 1 files into 19047 sequences of size 200\n",
      "Loading ML data from 1 files\n",
      "\n",
      "Transforming data into individual sequences...\n",
      "Transformed into 21609 sequences of size 200\n",
      "\n",
      "Combined 1 files into 21609 sequences of size 200\n",
      "Loading ML data from 1 files\n",
      "\n",
      "Transforming data into individual sequences...\n",
      "Transformed into 24467 sequences of size 200\n",
      "\n",
      "Combined 1 files into 24467 sequences of size 200\n"
     ]
    }
   ],
   "source": [
    "from data_ml import get_ml_data\n",
    "\n",
    "print(\"Data Preprocessing\")\n",
    "files_data_all = []\n",
    "files_labels_all = []\n",
    "\n",
    "for file_name in FILE_NAMES_ALL:\n",
    "    f_data_all, f_labels_all = get_ml_data(\n",
    "        events_path = EVENTS_PATH,\n",
    "        samples_path = SAMPLES_PATH,\n",
    "        file_names = [file_name],\n",
    "\n",
    "        event_id_map = EVENT_ID_MAP,\n",
    "        event_color_map = EVENT_COLOR_MAP,\n",
    "\n",
    "        event_sample_count = EVENT_SAMPLE_COUNT,\n",
    "        event_start = EVENT_START,\n",
    "        event_end = EVENT_END,\n",
    "\n",
    "        downsample_rate = DOWNSAMPLE_RATE,\n",
    "        shuffle_data = False,\n",
    "        filter_data = True,\n",
    "    )\n",
    "\n",
    "    files_data_all.append(f_data_all)\n",
    "    files_labels_all.append(f_labels_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RNgpn2dpFGW"
   },
   "source": [
    "### 3.3. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 200)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 200)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                12864     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                1040      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,972\n",
      "Trainable params: 13,972\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def get_model_ann():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=INPUT_SHAPE),\n",
    "        tf.keras.layers.Dropout(.50, input_shape=INPUT_SHAPE),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(16, activation='relu'),\n",
    "        tf.keras.layers.Dense(OUTPUT_SHAPE)\n",
    "    ])\n",
    "    return model\n",
    "get_model_ann().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "def get_model_svm():\n",
    "    clf = svm.SVC()\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import arange\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from xgboost import XGBRFClassifier\n",
    "\n",
    "#Hyper-Parameters\n",
    "n_trees = [10,50,100,500,1000,5000]\n",
    "n_features = [x for x in arange(0.1, 1.1, 0.1)]\n",
    "\n",
    "\n",
    "def get_model_rf(num_trees=50, num_features=0.1):\n",
    "    return XGBRFClassifier(n_estimators=num_trees, subsample=0.9, colsample_bynode=num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGYMbnF6pFJQ"
   },
   "source": [
    "### 3.4. Evaluation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN_EPOCHS = 3\n",
    "ANN_OPTIMIZER = 'adam'\n",
    "\n",
    "def train_model_ann(model, train_data, train_labels, test_data, test_labels):\n",
    "    print(\"Training ANN with\", len(train_labels), \"samples\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Trains model and returns history dict\n",
    "    model.compile(\n",
    "        optimizer=ANN_OPTIMIZER,\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        train_labels,\n",
    "        epochs=ANN_EPOCHS,\n",
    "        validation_data=(test_data, test_labels)\n",
    "    #     batch_size=16\n",
    "    )\n",
    "\n",
    "    print(f\"Completed training ANN in {time.time()-start_time:.2f}s\")\n",
    "\n",
    "    print(f\"Generating predictions for test set\", len(test_labels))\n",
    "    start_time = time.time()\n",
    "    \n",
    "    test_out = model.predict(test_data)\n",
    "    test_probs = tf.nn.softmax(test_out, axis=1)\n",
    "    test_pred = np.argmax(test_probs, axis=1)\n",
    "    \n",
    "    print(f\"Generated predictions in {time.time()-start_time:.2f}s\")\n",
    "\n",
    "    # Return predictions for test set\n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_svm(model, train_data, train_labels, test_data, test_labels):\n",
    "    subset = np.random.random(len(train_data)) < 0.1\n",
    "    print(\"Training SVM with\", sum(subset), \"samples\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.fit(train_data[subset], train_labels[subset])\n",
    "    \n",
    "    print(f\"Completed training SVM in {time.time()-start_time:.2f}s\")\n",
    "\n",
    "    print(f\"Generating predictions for test set\", len(test_labels))\n",
    "    start_time = time.time()\n",
    "    \n",
    "    test_pred = model.predict(test_data)\n",
    "\n",
    "    print(f\"Generated predictions in {time.time()-start_time:.2f}s\")\n",
    "    \n",
    "    # Return predicions for test set\n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Training and Testing\n",
    "def train_model_xgbrf(model, train_data, train_labels, test_data, test_labels):\n",
    "    subset = np.random.random(len(train_data)) < 0.1\n",
    "    print(\"Training XGBRandomForest with\", sum(subset), \"samples\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.fit(train_data[subset], train_labels[subset])\n",
    "    \n",
    "    print(f\"Completed training XGBRandomForest in {time.time()-start_time:.2f}s\")\n",
    "    \n",
    "    print(f\"Generating predictions for test set\", len(test_labels))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    test_pred = model.predict(test_data)\n",
    "    \n",
    "    print(f\"Generated predictions in {time.time()-start_time:.2f}s\")\n",
    "        \n",
    "    # Return predicions for test set\n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(label_pred, label_true):\n",
    "    # Rows are \"real\" labels\n",
    "    # Columns are \"predicted\" labels\n",
    "    conf = tf.math.confusion_matrix(\n",
    "        label_pred,\n",
    "        label_true\n",
    "    )\n",
    "\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1/3\n",
      "(46076, 200) (46076,)\n",
      "(19047, 200) (19047,)\n",
      "Training ANN with 46076 samples\n",
      "Epoch 1/3\n",
      "1440/1440 [==============================] - 2s 1ms/step - loss: 0.2168 - accuracy: 0.9214 - val_loss: 0.1872 - val_accuracy: 0.9411\n",
      "Epoch 2/3\n",
      "1440/1440 [==============================] - 2s 1ms/step - loss: 0.1349 - accuracy: 0.9482 - val_loss: 0.1574 - val_accuracy: 0.9505\n",
      "Epoch 3/3\n",
      "1440/1440 [==============================] - 2s 1ms/step - loss: 0.1142 - accuracy: 0.9558 - val_loss: 0.1588 - val_accuracy: 0.9450\n",
      "Completed training ANN in 5.01s\n",
      "Generating predictions for test set 19047\n",
      "596/596 [==============================] - 0s 496us/step\n",
      "Generated predictions in 0.45s\n",
      "Training SVM with 4574 samples\n",
      "Completed training SVM in 0.40s\n",
      "Generating predictions for test set 10000\n",
      "Generated predictions in 1.48s\n",
      "Training XGBRandomForest with 4657 samples\n",
      "Completed training XGBRandomForest in 65.82s\n",
      "Generating predictions for test set 10000\n",
      "Generated predictions in 0.62s\n",
      "Done fold in 73.98s\n",
      "\n",
      "Fold #2/3\n",
      "(43514, 200) (43514,)\n",
      "(21609, 200) (21609,)\n",
      "Training ANN with 43514 samples\n",
      "Epoch 1/3\n",
      "1360/1360 [==============================] - 2s 1ms/step - loss: 0.2363 - accuracy: 0.9134 - val_loss: 0.1693 - val_accuracy: 0.9420\n",
      "Epoch 2/3\n",
      "1360/1360 [==============================] - 2s 1ms/step - loss: 0.1401 - accuracy: 0.9463 - val_loss: 0.1640 - val_accuracy: 0.9461\n",
      "Epoch 3/3\n",
      "1360/1360 [==============================] - 2s 1ms/step - loss: 0.1172 - accuracy: 0.9546 - val_loss: 0.1537 - val_accuracy: 0.9464\n",
      "Completed training ANN in 5.10s\n",
      "Generating predictions for test set 21609\n",
      "676/676 [==============================] - 0s 500us/step\n",
      "Generated predictions in 0.50s\n",
      "Training SVM with 4369 samples\n",
      "Completed training SVM in 0.32s\n",
      "Generating predictions for test set 10000\n",
      "Generated predictions in 1.27s\n",
      "Training XGBRandomForest with 4327 samples\n",
      "Completed training XGBRandomForest in 61.21s\n",
      "Generating predictions for test set 10000\n",
      "Generated predictions in 0.64s\n",
      "Done fold in 69.37s\n",
      "\n",
      "Fold #3/3\n",
      "(40656, 200) (40656,)\n",
      "(24467, 200) (24467,)\n",
      "Training ANN with 40656 samples\n",
      "Epoch 1/3\n",
      "1271/1271 [==============================] - 2s 1ms/step - loss: 0.2272 - accuracy: 0.9177 - val_loss: 0.2070 - val_accuracy: 0.9347\n",
      "Epoch 2/3\n",
      "1271/1271 [==============================] - 1s 1ms/step - loss: 0.1324 - accuracy: 0.9488 - val_loss: 0.1995 - val_accuracy: 0.9325\n",
      "Epoch 3/3\n",
      "1271/1271 [==============================] - 1s 1ms/step - loss: 0.1157 - accuracy: 0.9543 - val_loss: 0.1823 - val_accuracy: 0.9425\n",
      "Completed training ANN in 4.83s\n",
      "Generating predictions for test set 24467\n",
      "765/765 [==============================] - 0s 495us/step\n",
      "Generated predictions in 0.57s\n",
      "Training SVM with 4091 samples\n",
      "Completed training SVM in 0.30s\n",
      "Generating predictions for test set 10000\n",
      "Generated predictions in 1.26s\n",
      "Training XGBRandomForest with 4224 samples\n",
      "Completed training XGBRandomForest in 59.49s\n",
      "Generating predictions for test set 10000\n",
      "Generated predictions in 0.61s\n",
      "Done fold in 67.26s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CV_K = len(FILE_NAMES_ALL)\n",
    "\n",
    "cv_conf_ann = []\n",
    "cv_conf_svm = []\n",
    "cv_conf_rf = []\n",
    "\n",
    "for k in range(CV_K):\n",
    "    print(f\"Fold #{k+1}/{CV_K}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get training data/labels\n",
    "    train_data = np.concatenate(files_data_all[0:k] + files_data_all[k+1:])\n",
    "    train_labels = np.concatenate(files_labels_all[0:k] + files_labels_all[k+1:])\n",
    "    print(train_data.shape, train_labels.shape)\n",
    "    \n",
    "    test_data = files_data_all[k]\n",
    "    test_labels = files_labels_all[k]\n",
    "    print(test_data.shape, test_labels.shape)\n",
    "\n",
    "    # Train and test ANN\n",
    "    model_ann = get_model_ann()\n",
    "    test_pred_ann = train_model_ann(model_ann, train_data, train_labels, test_data, test_labels)\n",
    "    conf_ann = confusion_matrix(test_pred_ann, test_labels)\n",
    "    cv_conf_ann.append(conf_ann)\n",
    "    \n",
    "    # Train and test SVM\n",
    "    model_svm = get_model_svm()\n",
    "    test_pred_svm = train_model_svm(model_svm, train_data, train_labels, test_data[:10000], test_labels[:10000])\n",
    "    conf_svm = confusion_matrix(test_pred_svm, test_labels[:10000])\n",
    "    cv_conf_svm.append(conf_svm)\n",
    "    \n",
    "    # Train and Test XGBRandomForest\n",
    "    model_xgbrf = get_model_rf()\n",
    "    test_pred_xgbrf = train_model_xgbrf(model_xgbrf, train_data, train_labels, test_data[:10000], test_labels[:10000])\n",
    "    conf_xgbrf = confusion_matrix(test_pred_xgbrf, test_labels[:10000])\n",
    "    cv_conf_rf.append(conf_xgbrf)\n",
    "\n",
    "    print(f\"Done fold in {time.time() - start_time:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
      "array([[16014,   225,   135,   246],\n",
      "       [  207,   523,     0,     9],\n",
      "       [  149,     0,   767,     2],\n",
      "       [   72,     2,     0,   696]])>, <tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
      "array([[18397,   208,   190,   172],\n",
      "       [  273,   994,     0,    13],\n",
      "       [  227,     0,   459,     0],\n",
      "       [   76,     0,     0,   600]])>, <tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
      "array([[20787,   170,   146,   458],\n",
      "       [  185,   742,     0,    69],\n",
      "       [  286,     0,  1307,     0],\n",
      "       [   53,    40,     0,   224]])>]\n",
      "[<tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
      "array([[8587,  149,   88,  264],\n",
      "       [  33,  151,    0,    0],\n",
      "       [  77,    0,  463,    0],\n",
      "       [   0,    0,    0,  188]])>, <tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
      "array([[8616,  116,  111,   83],\n",
      "       [ 126,  336,    0,    0],\n",
      "       [  48,    0,  288,    0],\n",
      "       [   8,    0,    0,  268]])>, <tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
      "array([[8650,  285,  129,  102],\n",
      "       [  15,  264,    0,   11],\n",
      "       [  30,    0,  422,    0],\n",
      "       [   3,    1,    0,   88]])>]\n",
      "[<tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
      "array([[8620,  218,  133,  301],\n",
      "       [  17,   65,    0,    0],\n",
      "       [  53,    0,  418,    0],\n",
      "       [   7,   17,    0,  151]])>, <tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
      "array([[8631,  134,  100,   73],\n",
      "       [  90,  318,    0,    0],\n",
      "       [  61,    0,  288,    0],\n",
      "       [  16,    0,   11,  278]])>, <tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
      "array([[8654,  304,  152,   97],\n",
      "       [  17,  237,    0,   13],\n",
      "       [  24,    0,  399,    0],\n",
      "       [   3,    9,    0,   91]])>]\n"
     ]
    }
   ],
   "source": [
    "print(cv_conf_ann)\n",
    "print(cv_conf_svm)\n",
    "print(cv_conf_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2YPuEpSrTrv"
   },
   "source": [
    "## 4. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxCTdo0_rTuH"
   },
   "source": [
    "### 4.1. Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(conf):\n",
    "    print(conf)\n",
    "    \n",
    "    overall_total = sum(sum(conf))\n",
    "    overall_correct = sum([conf[i][i] for i in range(len(conf))])\n",
    "    print(f\"Overall accuracy: {100*overall_correct/overall_total:.2f}% ({overall_correct}/{overall_total})\")\n",
    "    \n",
    "    fp = sum(conf[0][1:])\n",
    "    tp = sum(sum(conf[1:]))\n",
    "    print(f\"False positives {fp}\")\n",
    "    print(f\"True positives {tp}\")\n",
    "    print(f\"False discovery (fp/(tp+fp)): {fp/(fp+tp):.4f} ({fp}/{fp+tp})\")\n",
    "    \n",
    "    for i in range(len(conf)):\n",
    "        letter = str(EVENT_ID_LETTER_MAP[i])[0]\n",
    "        \n",
    "        total = sum(conf[i])\n",
    "        correct = conf[i][i]\n",
    "        acc_total = 100*correct/total\n",
    "        \n",
    "        s = f\"Event {letter} ({i}) accuracy: {correct:6}/{total: <6} (t_acc {acc_total:5.2f}%)\"\n",
    "        if i > 0:\n",
    "            acc_event = 100*correct/(total - conf[i][0])\n",
    "            s += f\" (e_acc {acc_event:5.2f}%)\"\n",
    "        print(s)\n",
    "        \n",
    "    print(\"\")\n",
    "        \n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55198   603   471   876]\n",
      " [  665  2259     0    91]\n",
      " [  662     0  2533     2]\n",
      " [  201    42     0  1520]]\n",
      "Overall accuracy: 94.45% (61510/65123)\n",
      "False positives 1950\n",
      "True positives 7975\n",
      "False discovery (fp/(tp+fp)): 0.1965 (1950/9925)\n",
      "Event N (0) accuracy:  55198/57148  (t_acc 96.59%)\n",
      "Event L (1) accuracy:   2259/3015   (t_acc 74.93%) (e_acc 96.13%)\n",
      "Event R (2) accuracy:   2533/3197   (t_acc 79.23%) (e_acc 99.92%)\n",
      "Event S (3) accuracy:   1520/1763   (t_acc 86.22%) (e_acc 97.31%)\n",
      "\n",
      "[[25853   550   328   449]\n",
      " [  174   751     0    11]\n",
      " [  155     0  1173     0]\n",
      " [   11     1     0   544]]\n",
      "Overall accuracy: 94.40% (28321/30000)\n",
      "False positives 1327\n",
      "True positives 2820\n",
      "False discovery (fp/(tp+fp)): 0.3200 (1327/4147)\n",
      "Event N (0) accuracy:  25853/27180  (t_acc 95.12%)\n",
      "Event L (1) accuracy:    751/936    (t_acc 80.24%) (e_acc 98.56%)\n",
      "Event R (2) accuracy:   1173/1328   (t_acc 88.33%) (e_acc 100.00%)\n",
      "Event S (3) accuracy:    544/556    (t_acc 97.84%) (e_acc 99.82%)\n",
      "\n",
      "[[25905   656   385   471]\n",
      " [  124   620     0    13]\n",
      " [  138     0  1105     0]\n",
      " [   26    26    11   520]]\n",
      "Overall accuracy: 93.83% (28150/30000)\n",
      "False positives 1512\n",
      "True positives 2583\n",
      "False discovery (fp/(tp+fp)): 0.3692 (1512/4095)\n",
      "Event N (0) accuracy:  25905/27417  (t_acc 94.49%)\n",
      "Event L (1) accuracy:    620/757    (t_acc 81.90%) (e_acc 97.95%)\n",
      "Event R (2) accuracy:   1105/1243   (t_acc 88.90%) (e_acc 100.00%)\n",
      "Event S (3) accuracy:    520/583    (t_acc 89.19%) (e_acc 93.36%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[25905,   656,   385,   471],\n",
       "       [  124,   620,     0,    13],\n",
       "       [  138,     0,  1105,     0],\n",
       "       [   26,    26,    11,   520]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_conf_ann = np.sum(cv_conf_ann, axis=0)\n",
    "print_confusion_matrix(total_conf_ann)\n",
    "\n",
    "total_conf_svm = np.sum(cv_conf_svm, axis=0)\n",
    "print_confusion_matrix(total_conf_svm)\n",
    "\n",
    "total_conf_rf = np.sum(cv_conf_rf, axis=0)\n",
    "print_confusion_matrix(total_conf_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9z3anWmrTwe"
   },
   "source": [
    "### 4.2. Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKFJra9urTyp"
   },
   "source": [
    "## 5. Discussion and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sb2q5DyrnY_"
   },
   "source": [
    "### 5.1. Issues Addressed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5Abg6FlrnbR"
   },
   "source": [
    "### 5.2. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGVcADPHrnd5"
   },
   "source": [
    "## 6. Student Contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0X4cmXHrngc"
   },
   "source": [
    "### 6.1. Matty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bafpxraprni8"
   },
   "source": [
    "### 6.2. Ashwin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zjRi9i3r9nn"
   },
   "source": [
    "### 6.3. Marcus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaTrmwj7r9qK"
   },
   "source": [
    "### 6.4. Alex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HS1Q0Pxr9sZ"
   },
   "source": [
    "### 6.5. Jingyu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ukHvavmr9ui"
   },
   "source": [
    "### 6.9. Josh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bZM8plir9xL"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8q57R1Hkr9zj"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjMRmECbr92D"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BoyYYZyr94d"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Paog5ZoCr97E"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pd0gRmUr99z"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkXy1VyMr-BO"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A02CJpxarnld"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OjUMV1vkrnoP"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Final Report.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
