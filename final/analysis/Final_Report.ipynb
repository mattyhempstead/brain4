{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiMZMZYYpFA9"
   },
   "source": [
    "# Final Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZZESMLfqCMX"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zd11uFJxp8Dd"
   },
   "source": [
    "## 1. Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ri2fXIvFp8GH"
   },
   "source": [
    "## 2. Aim and Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYqcpOKnp8Ig"
   },
   "source": [
    "### 2.1. Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZzWojd-rBmf"
   },
   "source": [
    "### 2.2. Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGFpT0m-rBxy"
   },
   "source": [
    "## 3. Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwcexQTIrB52"
   },
   "source": [
    "### 3.1. Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "l7Bv-0EM83z5"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aCgoTZLc59ic",
    "outputId": "94fa0c97-a3fd-4d31-f09f-3ffafa87af2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from data_plot import plot_samples_events_individual\n",
    "\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENT_ID_MAP = {\n",
    "    None: 0,\n",
    "    \"L\": 1,\n",
    "    \"R\": 2,\n",
    "    \"S\": 3,\n",
    "}\n",
    "EVENT_ID_LETTER_MAP = {EVENT_ID_MAP[i]:i for i in EVENT_ID_MAP}\n",
    "\n",
    "EVENT_COLOR_MAP = {\n",
    "    None: \"black\",\n",
    "    \"L\": \"red\",\n",
    "    \"R\": \"blue\",\n",
    "    \"S\": \"green\",\n",
    "}\n",
    "\n",
    "EVENT_ID_NAME_MAP = {\n",
    "    0: \"Nothing\",\n",
    "    1: \"Left Wink\",\n",
    "    2: \"Right Wink\",\n",
    "    3: \"Dbl Blink\",\n",
    "}\n",
    "\n",
    "BRAINBOX_SAMPLE_RATE = 10000\n",
    "DOWNSAMPLE_RATE = 100\n",
    "\n",
    "EVENT_LENGTH = 2 # length of a given event sequence in seconds\n",
    "EVENT_SAMPLE_COUNT = int(EVENT_LENGTH * BRAINBOX_SAMPLE_RATE / DOWNSAMPLE_RATE) # size of event in samples\n",
    "\n",
    "EVENT_START = -0.75\n",
    "EVENT_START_OFFSET = int(EVENT_START * BRAINBOX_SAMPLE_RATE / DOWNSAMPLE_RATE)\n",
    "\n",
    "EVENT_END = -0.25\n",
    "EVENT_END_OFFSET = int(EVENT_END * BRAINBOX_SAMPLE_RATE / DOWNSAMPLE_RATE)\n",
    "\n",
    "INPUT_SHAPE = (EVENT_SAMPLE_COUNT,)\n",
    "OUTPUT_SHAPE = len(EVENT_ID_MAP)  # number of categories (including None)\n",
    "\n",
    "\n",
    "EVENTS_PATH = \"../src/data_collection/data/events/\"\n",
    "SAMPLES_PATH = \"../src/data_collection/data/waves/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAMES_ALL = [\n",
    "    \"DATA_2022-05-13_Josh_0001_3_1652400625\",\n",
    "#     \"DATA_2022-05-13_Josh_0001_3_1652400939\",\n",
    "    \"DATA_2022-05-13_Josh_0001_4_1652401267\",\n",
    "#     \"DATA_2022-05-13_Josh_0001_4_1652401740\",\n",
    "    \"DATA_2022-05-13_Josh_0001_5_1652405337\",\n",
    "#     \"DATA_2022-05-13_Josh_0001_5_1652405637\",\n",
    "#     \"DATA_2022-05-13_Josh_0001_6_1652406023\",\n",
    "#     \"DATA_2022-05-13_Josh_0001_6_1652406202\",\n",
    "#     \"DATA_2022-05-13_Josh_0001_7_1652406589\",\n",
    "#     \"DATA_2022-05-13_Josh_0001_7_1652406788\",\n",
    "#     \"DATA_2022-05-13_Josh_0001_8_1652407331\",\n",
    "#     \"DATA_2022-05-13_Josh_0001_8_1652407508\",\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvrrURLQpFDk"
   },
   "source": [
    "### 3.2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessing\n",
      "Loading ML data from 1 files\n",
      "\n",
      "Transforming data into individual sequences...\n",
      "Transformed into 19047 sequences of size 200\n",
      "\n",
      "Combined 1 files into 19047 sequences of size 200\n",
      "Loading ML data from 1 files\n",
      "\n",
      "Transforming data into individual sequences...\n",
      "Transformed into 21609 sequences of size 200\n",
      "\n",
      "Combined 1 files into 21609 sequences of size 200\n",
      "Loading ML data from 1 files\n",
      "\n",
      "Transforming data into individual sequences...\n",
      "Transformed into 24467 sequences of size 200\n",
      "\n",
      "Combined 1 files into 24467 sequences of size 200\n"
     ]
    }
   ],
   "source": [
    "from data_ml import get_ml_data\n",
    "\n",
    "print(\"Data Preprocessing\")\n",
    "files_data_all = []\n",
    "files_labels_all = []\n",
    "\n",
    "for file_name in FILE_NAMES_ALL:\n",
    "    f_data_all, f_labels_all = get_ml_data(\n",
    "        events_path = EVENTS_PATH,\n",
    "        samples_path = SAMPLES_PATH,\n",
    "        file_names = [file_name],\n",
    "\n",
    "        event_id_map = EVENT_ID_MAP,\n",
    "        event_color_map = EVENT_COLOR_MAP,\n",
    "\n",
    "        event_sample_count = EVENT_SAMPLE_COUNT,\n",
    "        event_start = EVENT_START,\n",
    "        event_end = EVENT_END,\n",
    "\n",
    "        downsample_rate = DOWNSAMPLE_RATE,\n",
    "        shuffle_data = False,\n",
    "        filter_data = True,\n",
    "    )\n",
    "\n",
    "    files_data_all.append(f_data_all)\n",
    "    files_labels_all.append(f_labels_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RNgpn2dpFGW"
   },
   "source": [
    "### 3.3. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_52 (Flatten)        (None, 200)               0         \n",
      "                                                                 \n",
      " dropout_52 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " dense_156 (Dense)           (None, 64)                12864     \n",
      "                                                                 \n",
      " dense_157 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " dense_158 (Dense)           (None, 4)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,972\n",
      "Trainable params: 13,972\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def get_model_ann():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=INPUT_SHAPE),\n",
    "        tf.keras.layers.Dropout(.50, input_shape=INPUT_SHAPE),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(16, activation='relu'),\n",
    "        tf.keras.layers.Dense(OUTPUT_SHAPE)\n",
    "    ])\n",
    "    return model\n",
    "get_model_ann().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "def get_model_svm():\n",
    "    clf = svm.SVC()\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_rf():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGYMbnF6pFJQ"
   },
   "source": [
    "### 3.4. Evaluation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN_EPOCHS = 3\n",
    "ANN_OPTIMIZER = 'adam'\n",
    "\n",
    "def train_model_ann(model, train_data, train_labels, test_data, test_labels):\n",
    "    print(\"Training ANN with\", len(train_labels), \"samples\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Trains model and returns history dict\n",
    "    model.compile(\n",
    "        optimizer=ANN_OPTIMIZER,\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        train_labels,\n",
    "        epochs=ANN_EPOCHS,\n",
    "        validation_data=(test_data, test_labels)\n",
    "    #     batch_size=16\n",
    "    )\n",
    "\n",
    "    print(f\"Completed training ANN in {time.time()-start_time:.2f}s\")\n",
    "\n",
    "    print(f\"Generating predictions for test set\", len(test_labels))\n",
    "    start_time = time.time()\n",
    "    \n",
    "    test_out = model.predict(test_data)\n",
    "    test_probs = tf.nn.softmax(test_out, axis=1)\n",
    "    test_pred = np.argmax(test_probs, axis=1)\n",
    "    \n",
    "    print(f\"Generated predictions in {time.time()-start_time:.2f}s\")\n",
    "\n",
    "    # Return predictions for test set\n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_svm(model, train_data, train_labels, test_data, test_labels):\n",
    "    subset = np.random.random(len(train_data)) < 0.1\n",
    "    print(\"Training SVM with\", sum(subset), \"samples\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.fit(train_data[subset], train_labels[subset])\n",
    "    \n",
    "    print(f\"Completed training SVM in {time.time()-start_time:.2f}s\")\n",
    "\n",
    "    print(f\"Generating predictions for test set\", len(test_labels))\n",
    "    start_time = time.time()\n",
    "    \n",
    "    test_pred = model.predict(test_data)\n",
    "\n",
    "    print(f\"Generated predictions in {time.time()-start_time:.2f}s\")\n",
    "    \n",
    "    # Return predicions for test set\n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Random forest thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(label_pred, label_true):\n",
    "    # Rows are \"real\" labels\n",
    "    # Columns are \"predicted\" labels\n",
    "    conf = tf.math.confusion_matrix(\n",
    "        label_pred,\n",
    "        label_true\n",
    "    )\n",
    "\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1/3\n",
      "(46076, 200) (46076,)\n",
      "(19047, 200) (19047,)\n",
      "Training ANN with 46076 samples\n",
      "Epoch 1/3\n",
      "1440/1440 [==============================] - 2s 1ms/step - loss: 0.2485 - accuracy: 0.9099 - val_loss: 0.1952 - val_accuracy: 0.9396\n",
      "Epoch 2/3\n",
      "1440/1440 [==============================] - 2s 1ms/step - loss: 0.1478 - accuracy: 0.9446 - val_loss: 0.1538 - val_accuracy: 0.9501\n",
      "Epoch 3/3\n",
      "1440/1440 [==============================] - 2s 1ms/step - loss: 0.1251 - accuracy: 0.9527 - val_loss: 0.1568 - val_accuracy: 0.9490\n",
      "Completed training ANN in 5.40s\n",
      "Generating predictions for test set 19047\n",
      "Generated predictions in 0.33s\n",
      "Training SVM with 4559 samples\n",
      "Completed training SVM in 0.83s\n",
      "Generating predictions for test set 10000\n",
      "Generated predictions in 2.52s\n",
      "Done fold in 9.17s\n",
      "\n",
      "Fold #2/3\n",
      "(43514, 200) (43514,)\n",
      "(21609, 200) (21609,)\n",
      "Training ANN with 43514 samples\n",
      "Epoch 1/3\n",
      "1360/1360 [==============================] - 2s 1ms/step - loss: 0.2488 - accuracy: 0.9090 - val_loss: 0.1651 - val_accuracy: 0.9396\n",
      "Epoch 2/3\n",
      "1360/1360 [==============================] - 2s 1ms/step - loss: 0.1377 - accuracy: 0.9489 - val_loss: 0.1563 - val_accuracy: 0.9479\n",
      "Epoch 3/3\n",
      "1360/1360 [==============================] - 2s 1ms/step - loss: 0.1166 - accuracy: 0.9546 - val_loss: 0.1462 - val_accuracy: 0.9522\n",
      "Completed training ANN in 5.08s\n",
      "Generating predictions for test set 21609\n",
      "Generated predictions in 0.33s\n",
      "Training SVM with 4400 samples\n",
      "Completed training SVM in 0.77s\n",
      "Generating predictions for test set 10000\n",
      "Generated predictions in 2.41s\n",
      "Done fold in 8.67s\n",
      "\n",
      "Fold #3/3\n",
      "(40656, 200) (40656,)\n",
      "(24467, 200) (24467,)\n",
      "Training ANN with 40656 samples\n",
      "Epoch 1/3\n",
      "1271/1271 [==============================] - 2s 1ms/step - loss: 0.2331 - accuracy: 0.9163 - val_loss: 0.1943 - val_accuracy: 0.9360\n",
      "Epoch 2/3\n",
      "1271/1271 [==============================] - 2s 1ms/step - loss: 0.1384 - accuracy: 0.9471 - val_loss: 0.1905 - val_accuracy: 0.9387\n",
      "Epoch 3/3\n",
      "1271/1271 [==============================] - 2s 1ms/step - loss: 0.1188 - accuracy: 0.9528 - val_loss: 0.1896 - val_accuracy: 0.9384\n",
      "Completed training ANN in 4.99s\n",
      "Generating predictions for test set 24467\n",
      "Generated predictions in 0.38s\n",
      "Training SVM with 3996 samples\n",
      "Completed training SVM in 0.66s\n",
      "Generating predictions for test set 10000\n",
      "Generated predictions in 2.34s\n",
      "Done fold in 8.44s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CV_K = len(FILE_NAMES_ALL)\n",
    "\n",
    "cv_conf_ann = []\n",
    "cv_conf_svm = []\n",
    "# cv_conf_rf = []\n",
    "\n",
    "for k in range(CV_K):\n",
    "    print(f\"Fold #{k+1}/{CV_K}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get training data/labels\n",
    "    train_data = np.concatenate(files_data_all[0:k] + files_data_all[k+1:])\n",
    "    train_labels = np.concatenate(files_labels_all[0:k] + files_labels_all[k+1:])\n",
    "    print(train_data.shape, train_labels.shape)\n",
    "    \n",
    "    test_data = files_data_all[k]\n",
    "    test_labels = files_labels_all[k]\n",
    "    print(test_data.shape, test_labels.shape)\n",
    "\n",
    "    # Train and test ANN\n",
    "    model_ann = get_model_ann()\n",
    "    test_pred_ann = train_model_ann(model_ann, train_data, train_labels, test_data, test_labels)\n",
    "    conf_ann = confusion_matrix(test_pred_ann, test_labels)\n",
    "    cv_conf_ann.append(conf_ann)\n",
    "    \n",
    "    # Train and test SVM\n",
    "    model_svm = get_model_svm()\n",
    "    test_pred_svm = train_model_svm(model_svm, train_data, train_labels, test_data[:10000], test_labels[:10000])\n",
    "    conf_svm = confusion_matrix(test_pred_svm, test_labels[:10000])\n",
    "    cv_conf_svm.append(conf_svm)\n",
    "\n",
    "    print(f\"Done fold in {time.time() - start_time:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
      "array([[16092,   277,   118,   215],\n",
      "       [   76,   471,     0,     9],\n",
      "       [  172,     0,   784,     0],\n",
      "       [  102,     2,     0,   729]], dtype=int32)>, <tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
      "array([[18520,   275,   179,   125],\n",
      "       [  229,   927,     0,     0],\n",
      "       [  141,     0,   470,     0],\n",
      "       [   83,     0,     0,   660]], dtype=int32)>, <tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
      "array([[20910,   204,   301,   481],\n",
      "       [  239,   692,     0,    64],\n",
      "       [  132,     0,  1152,     0],\n",
      "       [   30,    56,     0,   206]], dtype=int32)>]\n",
      "[<tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
      "array([[8597,  166,   94,  244],\n",
      "       [  29,  134,    0,    0],\n",
      "       [  69,    0,  457,    0],\n",
      "       [   2,    0,    0,  208]], dtype=int32)>, <tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
      "array([[8642,  131,  135,   80],\n",
      "       [ 108,  321,    0,    0],\n",
      "       [  37,    0,  264,    0],\n",
      "       [  11,    0,    0,  271]], dtype=int32)>, <tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
      "array([[8636,  253,   83,  110],\n",
      "       [  15,  297,    0,   10],\n",
      "       [  47,    0,  468,    0],\n",
      "       [   0,    0,    0,   81]], dtype=int32)>]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(cv_conf_ann)\n",
    "print(cv_conf_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2YPuEpSrTrv"
   },
   "source": [
    "## 4. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxCTdo0_rTuH"
   },
   "source": [
    "### 4.1. Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(conf):\n",
    "    print(conf)\n",
    "    \n",
    "    overall_total = sum(sum(conf))\n",
    "    overall_correct = sum([conf[i][i] for i in range(len(conf))])\n",
    "    print(f\"Overall accuracy: {100*overall_correct/overall_total:.2f}% ({overall_correct}/{overall_total})\")\n",
    "    \n",
    "    fp = sum(conf[0][1:])\n",
    "    tp = sum(sum(conf[1:]))\n",
    "    print(f\"False positives {fp}\")\n",
    "    print(f\"True positives {tp}\")\n",
    "    print(f\"False discovery (fp/(tp+fp)): {fp/(fp+tp):.4f} ({fp}/{fp+tp})\")\n",
    "    \n",
    "    for i in range(len(conf)):\n",
    "        letter = str(EVENT_ID_LETTER_MAP[i])[0]\n",
    "        \n",
    "        total = sum(conf[i])\n",
    "        correct = conf[i][i]\n",
    "        acc_total = 100*correct/total\n",
    "        \n",
    "        s = f\"Event {letter} ({i}) accuracy: {correct:6}/{total: <6} (t_acc {acc_total:5.2f}%)\"\n",
    "        if i > 0:\n",
    "            acc_event = 100*correct/(total - conf[i][0])\n",
    "            s += f\" (e_acc {acc_event:5.2f}%)\"\n",
    "        print(s)\n",
    "        \n",
    "    print(\"\")\n",
    "        \n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55522   756   598   821]\n",
      " [  544  2090     0    73]\n",
      " [  445     0  2406     0]\n",
      " [  215    58     0  1595]]\n",
      "Overall accuracy: 94.61% (61613/65123)\n",
      "False positives 2175\n",
      "True positives 7426\n",
      "False discovery (fp/(tp+fp)): 0.2265 (2175/9601)\n",
      "Event N (0) accuracy:  55522/57697  (t_acc 96.23%)\n",
      "Event L (1) accuracy:   2090/2707   (t_acc 77.21%) (e_acc 96.63%)\n",
      "Event R (2) accuracy:   2406/2851   (t_acc 84.39%) (e_acc 100.00%)\n",
      "Event S (3) accuracy:   1595/1868   (t_acc 85.39%) (e_acc 96.49%)\n",
      "\n",
      "[[25875   550   312   434]\n",
      " [  152   752     0    10]\n",
      " [  153     0  1189     0]\n",
      " [   13     0     0   560]]\n",
      "Overall accuracy: 94.59% (28376/30000)\n",
      "False positives 1296\n",
      "True positives 2829\n",
      "False discovery (fp/(tp+fp)): 0.3142 (1296/4125)\n",
      "Event N (0) accuracy:  25875/27171  (t_acc 95.23%)\n",
      "Event L (1) accuracy:    752/914    (t_acc 82.28%) (e_acc 98.69%)\n",
      "Event R (2) accuracy:   1189/1342   (t_acc 88.60%) (e_acc 100.00%)\n",
      "Event S (3) accuracy:    560/573    (t_acc 97.73%) (e_acc 100.00%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[25875,   550,   312,   434],\n",
       "       [  152,   752,     0,    10],\n",
       "       [  153,     0,  1189,     0],\n",
       "       [   13,     0,     0,   560]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_conf_ann = np.sum(cv_conf_ann, axis=0)\n",
    "print_confusion_matrix(total_conf_ann)\n",
    "\n",
    "total_conf_svm = np.sum(cv_conf_svm, axis=0)\n",
    "print_confusion_matrix(total_conf_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9z3anWmrTwe"
   },
   "source": [
    "### 4.2. Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKFJra9urTyp"
   },
   "source": [
    "## 5. Discussion and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sb2q5DyrnY_"
   },
   "source": [
    "### 5.1. Issues Addressed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5Abg6FlrnbR"
   },
   "source": [
    "### 5.2. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGVcADPHrnd5"
   },
   "source": [
    "## 6. Student Contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0X4cmXHrngc"
   },
   "source": [
    "### 6.1. Matty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bafpxraprni8"
   },
   "source": [
    "### 6.2. Ashwin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zjRi9i3r9nn"
   },
   "source": [
    "### 6.3. Marcus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaTrmwj7r9qK"
   },
   "source": [
    "### 6.4. Alex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HS1Q0Pxr9sZ"
   },
   "source": [
    "### 6.5. Jingyu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ukHvavmr9ui"
   },
   "source": [
    "### 6.9. Josh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bZM8plir9xL"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8q57R1Hkr9zj"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjMRmECbr92D"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BoyYYZyr94d"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Paog5ZoCr97E"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pd0gRmUr99z"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkXy1VyMr-BO"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A02CJpxarnld"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OjUMV1vkrnoP"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Final Report.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
